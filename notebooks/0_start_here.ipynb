{
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark_pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dde02fa-0044-4b20-b7bb-10f1a5b3fabb",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": "### Data Engineering Pipelines with pandas on Snowflake\n\nThis demo is using the [Snowflake Sample TPC-H dataset](https://docs.snowflake.com/en/user-guide/sample-data-tpch) that should be in a shared database named `SNOWFLAKE_SAMPLE_DATA`. You can run this notebook in a Snowflake Notebook. \n\nDuring this demo you will learn how to use [pandas on Snowflake](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas) to:\n* Create datframe from a Snowflake table\n* ~Aggregate and transform data to create new features~\n* Save the result into a Snowflake table\n* Create a serverless task to schedule the feature engineering\n\npandas on Snowflake is delivered through the Snowpark pandas API as part of the Snowpark Python library (preinstalled with Snowflake Notebooks), which enables scalable data processing of Python code within the Snowflake platform. \n\nStart by adding neccessary libraries using the `Packages` dropdown, the additional libraries needed for this notebook is: \n* `modin` (select version 0.28.1)\n* `snowflake`\n* `matplotlib`\n* `seaborn`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039104e-54fc-411e-972e-0f5a2d884595",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66adbc4-2b92-4d7d-86a5-217ee78e061f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Snowpark Pandas API\n",
    "import modin.pandas as spd\n",
    "# Import the Snowpark pandas plugin for modin\n",
    "import snowflake.snowpark.modin.plugin\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "# Create a snowpark session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811abc04-f6b8-4ec4-8ad4-34af28ff8c31",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "# Name of the sample database and the schema to be used\n",
    "SOURCE_DATA_PATH = \"SNOWFLAKE_SAMPLE_DATA.TPCH_SF1\"\n",
    "SAVE_DATA_PATH = \"SNOW_PANDAS_DE_QS.DATA\"\n",
    "# Make sure we use the created database and schema for temp tables etc\n",
    "session.use_schema(SAVE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721a789-63a3-4c90-b763-50b8a1e69c92",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "We will start by creating a number of features based on the customer orders using the line items.\n",
    "\n",
    "Start with the `LINEITEM` table to create these features so we will start by creating a Snowpark Pandas Datframe aginst it, select the columns we are interested in and then show info about the dataframe, the shape and the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a091f1b-505f-4b61-9088-e7fd08e16f83",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "lineitem_keep_cols = ['L_ORDERKEY', 'L_LINENUMBER', 'L_PARTKEY', 'L_RETURNFLAG', 'L_QUANTITY', 'L_DISCOUNT', 'L_EXTENDEDPRICE']\n",
    "lineitem_df = spd.read_snowflake(f\"{SOURCE_DATA_PATH}.LINEITEM\")[lineitem_keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360d4de-21f4-4723-9778-ceb8683c81c8",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "st.dataframe(lineitem_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d37e2-e990-4e71-b762-41a64845955f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# Get info about the dataframe\n",
    "lineitem_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f45b8-a2a8-4d08-967e-945d2329335e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "print(f\"DataFrame shape: {lineitem_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fea0b-2f36-4662-a382-98938a74f2c2",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## Data Cleaning - Filtering and Aggregation\n",
    "\n",
    "Taking a look at different values for `L_RETURNFLAG` and include only line items that was delivered (`N`) or returned (`R`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f326c13-ed4c-4e6f-b40e-7e8338c270c4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "print(lineitem_df.L_RETURNFLAG.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cb06a-3a08-4d32-8864-4c8ff8c046b4",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "Add a filter to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c56b7-b2db-4591-97ce-451876e9b9a6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "print(f\"Before Filtering: {len(lineitem_df)} rows\")\n",
    "spd_lineitem = lineitem_df[lineitem_df['L_RETURNFLAG'] != 'A']\n",
    "print(f\"After Filtering: {len(spd_lineitem)} rows\")\n",
    "st.dataframe(spd_lineitem.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f802173-162f-4dff-8567-ade65b9f57f1",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "To track the actual discount a customer gets per order, we need to calculate that in a new column by taking the product of the amount of discount (`L_DISCOUNT`), numbers sold (`L_QUANTITY`), and the price of item (`L_EXTENDEDPRICE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f45f3d-3633-424e-b777-467a2ba0b22d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "spd_lineitem['DISCOUNT_AMOUNT'] = spd_lineitem['L_DISCOUNT'] * spd_lineitem['L_QUANTITY'] * spd_lineitem['L_EXTENDEDPRICE']\n",
    "st.dataframe(spd_lineitem.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9d862-e957-42b9-9d86-03f2ad3501f7",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": [
    "Now we want to compute the aggregate of items and discount amount, grouped by order key and return flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cbdf7-a655-416b-87da-417f7edd35bb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# Aggregations we want to do\n",
    "column_agg = {\n",
    "                'L_QUANTITY':['sum'], # Total Items Ordered  \n",
    "                'DISCOUNT_AMOUNT': ['sum'] # Total Discount Amount\n",
    "             }\n",
    "\n",
    "# Apply the aggregation\n",
    "spd_lineitem_agg = spd_lineitem.groupby(by=['L_ORDERKEY', 'L_RETURNFLAG'], as_index=False).agg(column_agg)\n",
    "\n",
    "# Rename the columns\n",
    "spd_lineitem_agg.columns = ['L_ORDERKEY', 'L_RETURNFLAG', 'NBR_OF_ITEMS', 'TOT_DISCOUNT_AMOUNT']\n",
    "st.dataframe(spd_lineitem_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd1299-9bb2-4aba-9f37-b04ca3639892",
   "metadata": {
    "collapsed": false,
    "name": "cell18"
   },
   "source": [
    "## Data Transformation - Pivot and reshape\n",
    "\n",
    "We want to separate the `NBR_OF_ITEMS` and `TOT_DISCOUNT_AMOUNT` by `L_RETURNFLAG` so we have one column for each uinique `L_RETURNFLAG` value.  \n",
    "Using the **pivot_table** method will give us one column for each unique value in `RETURN_FLAG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f586e8a-017b-4672-80a1-bcc9430a87c3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# This will make L_ORDERKEY the index\n",
    "spd_lineitem_agg_pivot_df = spd_lineitem_agg.pivot_table(\n",
    "                                values=['NBR_OF_ITEMS', 'TOT_DISCOUNT_AMOUNT'], \n",
    "                                index=['L_ORDERKEY'],\n",
    "                                columns=['L_RETURNFLAG'], \n",
    "                                aggfunc=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd144f-b18b-4673-b8c0-7db6d237ae59",
   "metadata": {
    "collapsed": false,
    "name": "cell20"
   },
   "source": [
    "The **pivot_table** method returns subcolumns and by renaming the columns we will get rid of those, and have one unique columns for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166f8b0-fc8c-451e-9780-3e1f634ccbdd",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "spd_lineitem_agg_pivot_df.columns = ['NBR_OF_ITEMS_N', 'NBR_OF_ITEMS_R','TOT_DISCOUNT_AMOUNT_N','TOT_DISCOUNT_AMOUNT_R']\n",
    "# Move L_ORDERKEY back to column\n",
    "spd_lineitem_agg_pivot = spd_lineitem_agg_pivot_df.reset_index(names=['L_ORDERKEY'])\n",
    "st.dataframe(spd_lineitem_agg_pivot.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657bbc7-caf2-461c-9302-6f8d2187e0af",
   "metadata": {
    "collapsed": false,
    "name": "cell22"
   },
   "source": [
    "## Combine lineitem with orders information\n",
    "\n",
    "Load `ORDERS` table and join with dataframe with transformed lineitem information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910ac10-38b3-4aa4-a7d2-6321243a4a60",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "spd_order = spd.read_snowflake(f\"{SOURCE_DATA_PATH}.ORDERS\")\n",
    "# Drop unused columns \n",
    "spd_order = spd_order.drop(['O_ORDERPRIORITY', 'O_CLERK', 'O_SHIPPRIORITY', 'O_COMMENT'], axis=1)\n",
    "# Use streamlit to display the dataframe\n",
    "st.dataframe(spd_order.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d52cd4-a71b-4c72-9137-accdf54b571b",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": [
    "Use **merge** to join the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee6f94-f33b-4492-9f89-2808c05f07d4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# Join dataframes\n",
    "spd_order_items = spd_lineitem_agg_pivot.merge(spd_order,\n",
    "                                               left_on='L_ORDERKEY', \n",
    "                                               right_on='O_ORDERKEY', \n",
    "                                               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc0331-1879-452f-9cc6-dd69f6824974",
   "metadata": {
    "collapsed": false,
    "name": "cell26"
   },
   "source": [
    "Drop the `L_ORDERKEY`column, it has the same values as `O_ORDERKEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504a44d-d687-4c8d-af78-4b802901a168",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "spd_order_items.drop('L_ORDERKEY', axis=1, inplace=True)\n",
    "st.write(f\"DataFrame shape: {spd_order_items.shape}\")\n",
    "st.dataframe(spd_order_items.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b050f9-77a9-460a-853b-888963e6a214",
   "metadata": {
    "collapsed": false,
    "name": "cell28"
   },
   "source": [
    "More aggregations grouped by customer (`O_CUSTKEY`)\n",
    "* Total items delivered by customer\n",
    "* Average items delivered by customer\n",
    "* Total items returned by customer\n",
    "* Average items returned by customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e32341-cc93-4b5d-a5f1-15a15d8ddf69",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "# Aggregations we want to do\n",
    "column_agg = {\n",
    "                'O_ORDERKEY':['count'], \n",
    "                'O_TOTALPRICE': ['sum' ,'mean', 'median'],\n",
    "                'NBR_OF_ITEMS_N': ['sum' ,'mean', 'median'],\n",
    "                'NBR_OF_ITEMS_R': ['sum' ,'mean', 'median'],\n",
    "                'TOT_DISCOUNT_AMOUNT_N': ['sum'],\n",
    "                'TOT_DISCOUNT_AMOUNT_R': ['sum']\n",
    "            }\n",
    "\n",
    "# Apply the aggregation\n",
    "spd_order_profile = spd_order_items.groupby(by='O_CUSTKEY', as_index=False).agg(column_agg)\n",
    "\n",
    "# Rename the columns\n",
    "spd_order_profile.columns = ['O_CUSTKEY', 'NUMBER_OF_ORDERS', 'TOT_ORDER_AMOUNT', 'AVG_ORDER_AMOUNT', 'MEDIAN_ORDER_AMOUNT', \n",
    "                             'TOT_ITEMS_DELIVERED', 'AVG_ITEMS_DELIVERED', 'MEDIAN_ITEMS_DELIVERED', \n",
    "                             'TOT_ITEMS_RETURNED', 'AVG_ITEMS_RETURNED', 'MEDIAN_ITEMS_RETURNED',\n",
    "                             'TOT_DISCOUNT_AMOUNT_N', 'TOT_DISCOUNT_AMOUNT_R']\n",
    "st.dataframe(spd_order_profile.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0e441-43d1-4729-bc20-aea8f123befa",
   "metadata": {
    "collapsed": false,
    "name": "cell30"
   },
   "source": [
    "Calculate the total and average discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca62db-8d7b-4d16-89e6-515bcd2cfd7b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "spd_order_profile['TOT_DISCOUNT'] = spd_order_profile['TOT_DISCOUNT_AMOUNT_N'] + spd_order_profile['TOT_DISCOUNT_AMOUNT_R']\n",
    "spd_order_profile['AVG_DISCOUNT'] = spd_order_profile['TOT_DISCOUNT'] / spd_order_profile['NUMBER_OF_ORDERS']\n",
    "st.dataframe(spd_order_profile.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b78081-9ed9-48db-b7d7-47cdbe3e499b",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "## Saving results to Snowflake Table\n",
    "\n",
    "We can now save our customer profile as a Snowflake table, in this case we will replace it if it already exists and by setting `index=False` we do not save the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7789b8f-a096-450f-85a7-c9e393c1d51f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": [
    "spd_order_profile.to_snowflake(name=f\"{SAVE_DATA_PATH}.customer_profile\", if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a919f-1a6f-49ec-b786-8ec1645787c1",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": [
    "We can check using SQL that we have data in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacead4-dba7-4b35-ad0b-23dc173aada2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM {{SAVE_DATA_PATH}}.customer_profile LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e95e5f-aed0-43cc-92aa-a976e652c5c1",
   "metadata": {
    "collapsed": false,
    "name": "cell36"
   },
   "source": [
    "## Visualize data distribution\n",
    "\n",
    "Plot histogram distribution for different columns in customer profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467d0df-50d0-4dec-8149-d14c8c897d3c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "spd_profile =  spd.read_snowflake(f\"{SAVE_DATA_PATH}.customer_profile\")  \n",
    "pd_profile = spd_profile.to_pandas()\n",
    "\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,3))\n",
    "\n",
    "colnames = ['NUMBER_OF_ORDERS', 'AVG_ORDER_AMOUNT', 'AVG_ITEMS_DELIVERED', 'AVG_ITEMS_RETURNED']\n",
    "# Iterating through axes and names\n",
    "for col, ax in zip(colnames, axes.flatten()):\n",
    "    ax.set_title(col)\n",
    "    sns.histplot(pd_profile, x=col , ax=ax, kde=True, stat=\"density\", kde_kws=dict(cut=3), alpha=.4, edgecolor=(1, 1, 1, .4))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df02f3-00ae-4b9c-9b8b-d5d46815ff40",
   "metadata": {
    "collapsed": false,
    "name": "cell38"
   },
   "source": [
    "## Orchestrate Data Pipeline: Scheduling with Serverless Tasks\n",
    "\n",
    "We have now used Snowpark Pandas API to create a Customer profile based on their purchase data.\n",
    "\n",
    "A next step is to run this notebook regulary to update the profiles when we have new data, this can be done by scheduling it using the schedule function in notebooks or using a CI/CD pipeline.  \n",
    "\n",
    "Another way is to create a serverless task directly in the notebook. In order to do that we need to create a Python function with all the steps we have done in so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d3289-e022-432f-96d3-d105c3b38f79",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": [
    "# Snowflake Python API, to be used to create a serverless task\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.task import Task\n",
    "from snowflake.core import CreateMode\n",
    "from snowflake.snowpark import Session\n",
    "root = Root(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc2c52-959f-4198-8c23-cd8b564e972e",
   "metadata": {
    "collapsed": false,
    "name": "cell40"
   },
   "source": [
    "Convert our Snowpark pandas data pipeline from earlier to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca6303-eae9-4e14-bea8-666e6d286b22",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "def create_customer_profile(snf_session: snowflake.snowpark.Session, data_path: str, save_data_path: str) -> str:\n",
    "    from datetime import datetime\n",
    "    #  Get line item\n",
    "    lineitem_keep_cols = ['L_ORDERKEY', 'L_LINENUMBER', 'L_PARTKEY', 'L_RETURNFLAG', 'L_QUANTITY', 'L_DISCOUNT', 'L_EXTENDEDPRICE']\n",
    "    spd_lineitem = spd.read_snowflake(f\"{data_path}.LINEITEM\")[lineitem_keep_cols]\n",
    "    spd_lineitem = spd_lineitem[spd_lineitem['L_RETURNFLAG'] != 'A']\n",
    "    spd_lineitem['DISCOUNT_AMOUNT'] = (spd_lineitem['L_DISCOUNT'] * (spd_lineitem['L_QUANTITY'] * spd_lineitem['L_EXTENDEDPRICE']))\n",
    "    \n",
    "    # Aggregations we want to do on line item\n",
    "    column_agg = {\n",
    "                    'L_QUANTITY':['sum'], # Total Items Ordered  \n",
    "                    'DISCOUNT_AMOUNT': ['sum'] # Total Discount Amount\n",
    "                 }\n",
    "    \n",
    "    # Apply the aggregation\n",
    "    spd_lineitem_agg = spd_lineitem.groupby(by=['L_ORDERKEY', 'L_RETURNFLAG'], as_index=False).agg(column_agg)\n",
    "    \n",
    "    # Rename the columns\n",
    "    spd_lineitem_agg.columns = ['L_ORDERKEY', 'L_RETURNFLAG', 'NBR_OF_ITEMS', 'TOT_DISCOUNT_AMOUNT']\n",
    "    \n",
    "    # # This will make L_ORDERKEY the index\n",
    "    spd_lineitem_agg_pivot = spd_lineitem_agg.pivot_table(values=['NBR_OF_ITEMS', 'TOT_DISCOUNT_AMOUNT'], index=['L_ORDERKEY'],\n",
    "                            columns=['L_RETURNFLAG'], aggfunc=\"sum\")\n",
    "    # Pivot the dataframe\n",
    "    spd_lineitem_agg_pivot.columns = ['NBR_OF_ITEMS_N', 'NBR_OF_ITEMS_R','TOT_DISCOUNT_AMOUNT_N','TOT_DISCOUNT_AMOUNT_R']\n",
    "    \n",
    "    # # Move L_ORDERKEY back to column\n",
    "    spd_lineitem_agg_pivot.reset_index(names=['L_ORDERKEY'], inplace=True)\n",
    "\n",
    "    # Get Orders\n",
    "    spd_order = spd.read_snowflake(f\"{data_path}.ORDERS\")\n",
    "    # Drop unused columns \n",
    "    spd_order = spd_order.drop(['O_ORDERPRIORITY', 'O_CLERK', 'O_SHIPPRIORITY', 'O_COMMENT'], axis=1)\n",
    "\n",
    "    # Join orders with the pivoted lineitems\n",
    "    spd_order_items = spd_lineitem_agg_pivot.merge(spd_order, left_on='L_ORDERKEY', right_on='O_ORDERKEY', how='left')\n",
    "    spd_order_items.drop('L_ORDERKEY', axis=1, inplace=True)\n",
    "    \n",
    "    # Aggregations we want to do\n",
    "    column_agg = {\n",
    "                    'O_ORDERKEY':['count'], \n",
    "                    'O_TOTALPRICE': ['sum' ,'mean', 'median'],\n",
    "                    'NBR_OF_ITEMS_N': ['sum' ,'mean', 'median'],\n",
    "                    'NBR_OF_ITEMS_R': ['sum' ,'mean', 'median'],\n",
    "                    'TOT_DISCOUNT_AMOUNT_N': ['sum'],\n",
    "                    'TOT_DISCOUNT_AMOUNT_R': ['sum']\n",
    "                }\n",
    "    \n",
    "    # Apply the aggregation\n",
    "    spd_order_profile = spd_order_items.groupby(by='O_CUSTKEY', as_index=False).agg(column_agg)\n",
    "    \n",
    "    # Rename the columns\n",
    "    spd_order_profile.columns = ['O_CUSTKEY', 'NUMBER_OF_ORDERS', 'TOT_ORDER_AMOUNT', 'AVG_ORDER_AMOUNT', 'MEDIAN_ORDER_AMOUNT', \n",
    "                                 'TOT_ITEMS_DELIVERED', 'AVG_ITEMS_DELIVERED', 'MEDIAN_ITEMS_DELIVERED', \n",
    "                                 'TOT_ITEMS_RETURNED', 'AVG_ITEMS_RETURNED', 'MEDIAN_ITEMS_RETURNED',\n",
    "                                 'TOT_DISCOUNT_AMOUNT_N', 'TOT_DISCOUNT_AMOUNT_R']\n",
    "    \n",
    "    # Calculate total and average                      \n",
    "    spd_order_profile['TOT_DISCOUNT'] = spd_order_profile['TOT_DISCOUNT_AMOUNT_N'] + spd_order_profile['TOT_DISCOUNT_AMOUNT_R']\n",
    "    spd_order_profile['AVG_DISCOUNT'] = spd_order_profile['TOT_DISCOUNT'] / spd_order_profile['NUMBER_OF_ORDERS']\n",
    "    \n",
    "    # Save to a table, replace if existing\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    save_path = f\"{save_data_path}.customer_profile_{timestamp}\"\n",
    "    spd_order_profile.to_snowflake(name=save_path, if_exists=\"replace\", index=False)    \n",
    "    return f'Successful run with Modin:{spd.__version__}, Snowpark:{snowflake.snowpark.__version__}. Saved to {save_path}.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3985235-09cf-4063-bae4-1f935821454c",
   "metadata": {
    "collapsed": false,
    "name": "cell42"
   },
   "source": [
    "Create and register a stored procedure based on the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9fcc7-9a7a-49c3-b9e1-0749ead15013",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "CREATE STAGE IF NOT EXISTS task_code_stage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7cf5e-6792-456d-bb5e-fa3c9735be19",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "sp_customer_profile = session.sproc.register(name=\"create_customer_profile_sp\", \n",
    "                                             func=create_customer_profile, replace=True, \n",
    "                                             is_permanent=True, \n",
    "                                             packages=['modin', 'snowflake-snowpark-python'], \n",
    "                                             stage_location='@task_code_stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f7b9b-35a7-4579-8b2e-27814cd31445",
   "metadata": {
    "collapsed": false,
    "name": "cell45"
   },
   "source": [
    "Here's an example of how you can call the stored procedure manually:\n",
    "```sql\n",
    "CALL create_customer_profile_sp('{{SOURCE_DATA_PATH}}', '{{SAVE_DATA_PATH}}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35decf1-eb23-480e-a2ac-3677a9e4199b",
   "metadata": {
    "collapsed": false,
    "name": "cell46"
   },
   "source": [
    "Rather than calling this manually, we will create a serverless task that calls the stored procedure. The task is set on a schedule to run once every minute. Note how we do not need to specify a warehouse size for Serverless Tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd641546-9a21-474e-90c8-96914a99d6ff",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "# Register data pipeline function as a task\n",
    "my_task = Task(name='create_customer_profile_task',\n",
    "               definition=f\"CALL create_customer_profile_sp('{SOURCE_DATA_PATH}', '{SAVE_DATA_PATH}')\",\n",
    "               schedule=timedelta(minutes=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4423c01-72bc-41ae-bc2f-25ebf7591a86",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": [
    "\n",
    "DB_NAME = SAVE_DATA_PATH.split(\".\")[0]\n",
    "SCHEMA_NAME = SAVE_DATA_PATH.split(\".\")[1]\n",
    "tasks = root.databases[DB_NAME].schemas[SCHEMA_NAME].tasks\n",
    "task_res = tasks.create(my_task,mode=CreateMode.or_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745afc02-5483-45a7-b559-33941e8d2701",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell49"
   },
   "outputs": [],
   "source": [
    "SHOW TASKS LIKE '%CUSTOMER_PROFILE%' IN SCHEMA {{SAVE_DATA_PATH}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795a6ad-e025-4a19-b245-b9e2dbbc5154",
   "metadata": {
    "collapsed": false,
    "name": "cell50"
   },
   "source": [
    "By default, new tasks that are created are suspended, so we resume this to get the task to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b86364-2eca-48c2-803b-09a71a043389",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "task_res.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7992b-9b53-4ac7-bf73-9e9625ae7583",
   "metadata": {
    "collapsed": false,
    "name": "cell52"
   },
   "source": [
    "While we are waiting for this task to run, let's take a look at a few things: \n",
    "- If your notebook is connected to Git, you can commit changes to notebook with Git integration. Make an edit to your notebook and click on \"Push to Git\" button on the top right to commit and push in the changes. You should see the committed changes Once the changes are pushed in, you will see the changes in your remote commit history. To learn more about Git with Notebooks, see [this video](https://www.youtube.com/watch?v=4GOa1eUccmQ).\n",
    "- You can navigate to the object explorer on the left hand pane under \"Data\" and search for the `CREATE_CUSTOMER_PROFILE_TASK`. By clicking on the task, you can look at the Task Details, Graph, and Run History in Snowsight directly.\n",
    "\n",
    "Now let's take a look at the task history and the status on the task runs using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8141e33-2188-4dad-886d-a516127ed616",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell53"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TABLE({{DB_NAME}}.information_schema.task_history(task_name=> 'create_customer_profile_task'))\n",
    "WHERE SCHEDULED_TIME >= CURRENT_TIMESTAMP() - INTERVAL '10 MINUTES';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa275b1-0158-4fbb-9f44-d5e766690e62",
   "metadata": {
    "collapsed": false,
    "name": "cell54"
   },
   "source": [
    "Once the runs have completed, you will see the new table with the timestamp being created.\n",
    "Note that Notebooks also support scheduling with Tasks through the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c82ab-9d7b-4a84-8820-ec31579c80ab",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell55"
   },
   "outputs": [],
   "source": [
    "SHOW TABLES LIKE 'CUSTOMER_PROFILE_%' IN {{SAVE_DATA_PATH}};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fcafb-aeb9-4846-b99a-c95d177adf06",
   "metadata": {
    "collapsed": false,
    "name": "cell56"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "Using Python API, I can suspend the task so that it stops running on the schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce71a50-3c06-472d-879d-77cc68478094",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell57"
   },
   "outputs": [],
   "source": [
    "task_res.suspend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143fa85-8dce-4f53-b7a1-bc3de8f17553",
   "metadata": {
    "collapsed": false,
    "name": "cell58"
   },
   "source": [
    " Teardown the tables created from the Tasks to clean up my environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce11d04-5f1b-476a-96be-c18cb3011856",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": [
    "tables = root.databases[DB_NAME].schemas[SCHEMA_NAME].tables.iter(like='CUSTOMER_PROFILE_%')\n",
    "for table in tables:\n",
    "    my_table_res = root.databases[DB_NAME].schemas[SCHEMA_NAME].tables[table.name]\n",
    "    my_table_res.delete()\n",
    "    print(f\"Deleted {table.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81572d2-b9f8-42ba-916f-95d8a4e07e5b",
   "metadata": {
    "collapsed": false,
    "name": "cell60"
   },
   "source": [
    "Verify that tables have been dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162230f1-ef09-41b1-bf04-21b73a1d25f0",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell61"
   },
   "outputs": [],
   "source": [
    "SHOW TABLES LIKE 'CUSTOMER_PROFILE_%' IN {{SAVE_DATA_PATH}};"
   ]
  }
 ]
}